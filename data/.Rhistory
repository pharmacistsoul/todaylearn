SimplePos22(sentence)
rm(list=ls())
rm(list=ls())
library(KoNLP)
# 03-02 사전 불러오기
useSejongDic()
# 03-03 명사 추출
# extractNoun(문장)
# Hannanum analyzer 을 사용해서 한국어 문장으로부터 명사를 추출
# Java로 개발
# autoSpacing : 입력에 대한 자동 공백
sentence <- "우리나라 고유의 문화는 우수하다. 전시회를 재미있게 보았다. 시간이 스르륵 지나갔다."
extractNoun(sentence, autoSpacing = FALSE)  # 기본
extractNoun(sentence, autoSpacing = TRUE)
# 03-04 사전에 단어 추가
# mergeUserDic() 함수를 이용해서 sejong 사전에 '스르륵' 단어 '부사'로 추가
# http://github.com/haven-jeon/KoNLP/blob/master/etcs/KoNLP-API.md
mergeUserDic(data.frame(c('스르륵'),c('mag') ))
extractNoun(sentence)
# 03-05 형태소 분석하기
# Morpheme : 형태소
# MorphAnalyzer()
sentence
MorphAnalyzer(sentence)  # 형태소 분석 결과 산출
# 03-06
SimplePos09(sentence)
SimplePos22(sentence)
library(KoNLP)
search()
useSejongDic()
sentence <- "우리나라 고유의 문화들을 멋진그래픽으로 잘 표현했다. 재미있게 봤다. 스르륵 시간이 지나갔다."
sentence2 <- "님은 갔습니다. 아아, 사랑하는 나의 님은 갔습니다.
푸른 산빛을 깨치고 단풍나무 숲을 향하여 난 작은 길을 걸어서 차마 떨치고 갔습니다.
황금의 꽃같이 굳고 빛나던 옛 맹세는 차디찬 티끌이 되어서 한숨의 미풍에 날아갔습니다.
날카로운 첫 키스의 추억은 나의 운명의 지침을 돌려 놓고 뒷걸음쳐서 사라졌습니다.
나는 향기로운 님의 말소리에 귀먹고 꽃다운 님의 얼굴에 눈멀었습니다."
extractNoun(sentence)
extractNoun(sentence2)
### 스르륵 : 부사
mergeUserDic(data.frame(c('스르륵'), c('mag')))
extractNoun(sentence)
### 스르륵 : 부사
mergeUserDic(data.frame(c('스르륵'), c('mag')))
extractNoun(sentence)
### 형태소 분석하기
### MorphAnalyzer(sentence)
MorphAnalyzer(sentence)
SimplePos09(sentence)
SimplePos22(sentence)
### tmPackage
### MeCAB
###
library(tm)
docs <- c("I am boy" , "You are a girl", "I am a student")
is(docs)
print(Corpus(VectorSource(docs)))
## doc1 : "I am boy"
## doc2 : "You are a girl"
## doc3 ; "I am a student"
myCorpus = Corpus(VectorSource(docs))
inspect(myCorpus)
# 데이터 전처리
myCorpus <- tm_map(myCorpus, stripWhitespace)                   ## 공백제거
myCorpus <- tm_map(myCorpus, tolower)                           ## 소문자로 만들기
myCorpus <- tm_map(myCorpus, removePunctuation)                 ## 특수문자제거
myCorpus <- tm_map(myCorpus, removeNumbers)                     ## 숫자 제거
myCorpus <- tm_map(myCorpus, removeWords, stopwords("english")) ## 불용어 제거
tdm <- TermDocumentMatrix(myCorpus)
tdm
m2 <- as.matrix(tdm)
m2
##
wordFreq <- sort(rowSums(m2), decreasing=TRUE)
head(wordFreq, 20)
## 빈도확인, 연관성 확인
findFreqTerms(tdm, lowfreq=1, highfreq=Inf)
findAssocs(tdm, "you", 0.2)
## meta 정보를 추가하겠다.
meta(myCorpus[[3]], tag='etc') <- "메타정보추가"
meta(myCorpus[[3]])
library(tm)
dat <- readLines("C:/Users/ktm/Desktop/빅데이터/dir_multi/ratings01.txt")
print(dat)
# 03. 텍스트에서 말뭉치(Corpus)로 변환
tSource <- VectorSource(dat)
tSource
myCor <- Corpus(tSource)
myCor
# 05. 전처리가 후, 문서 내용 확인
# tm_map( tolower )
myCorpus <- tm_map(myCorpus, removeNumbers)                     # 숫자 제거
myCorpus <- tm_map(myCorpus, removePunctuation)                 # 특수 문자 제거
myCorpus <- tm_map(myCorpus, stripWhitespace)                   # 공백 제거
inspect(myCor)
myCor[[3]]$content
myCorpus <- tm_map(myCorpus, removewords)
myCorpus <- tm_map(myCorpus, removeWords)
myCorpus <- tm_map(myCorpus, removeWords)
myCorpus <- tm_map(myCorpus, removeWords, stopwords(""))
myCorpus
# 07. 단어 문서 행렬(Term Document Matrix, TDM)
# TF(용어의 빈도수)
# TFIDF
tdm <- TermDocumentMatrix(myCor, control=list(tokenize="scan", wordLengths=c(2,7)))
inspect(tdm)
tdm_M <- as.matrix(tdm)
tdm_M
## 단어 빈도수 구하기
frequency <- rowSums(tdm_M)
frequency
## 정렬
frequency <- sort(frequency, decreasing = T)
frequency
barplot(frequency[1:20], las=2)
library(tm)
dat <- readLines("C:/Users/ktm/Desktop/빅데이터/comment.txt")
print(dat)
print(dat)
tSource <- VectorSource(dat)
tSource
myCor <- Corpus(tSource)
myCor
# 05. 전처리가 후, 문서 내용 확인
# tm_map( tolower )
myCorpus <- tm_map(myCorpus, removeNumbers)                     # 숫자 제거
myCorpus <- tm_map(myCorpus, removePunctuation)                 # 특수 문자 제거
myCorpus <- tm_map(myCorpus, stripWhitespace)                   # 공백 제거
inspect(myCor)
myCor[[3]]$content
# 06. 불용어 처리 수행
## stopwords : C:/Users/ktm/Documents/R/win-library/3.5/tm/stopwords
## 나의 라이브러리 위치
.libPaths()
myCorpus <- tm_map(myCorpus, removeWords, stopwords(""))
# 07. 단어 문서 행렬(Term Document Matrix, TDM)
# TF(용어의 빈도수)
# TFIDF
tdm <- TermDocumentMatrix(myCor, control=list(tokenize="scan", wordLengths=c(2,7)))
inspect(tdm)
tdm_M <- as.matrix(tdm)
tdm_M
## 단어 빈도수 구하기
frequency <- rowSums(tdm_M)
frequency
## 정렬
frequency <- sort(frequency, decreasing = T)
frequency    # 단어별 빈도
barplot(frequency[1:20], las=2)   # 막대그래프 그리고
library(KoNLP)
library(NIADic)
useNIADic()
removeWord = c("영화", "ㅋ", "아무런", "있는", "여러", "바보", "더욱", "엉뚱하면서도")
library(tm)
dat <- readLines("C:/Users/ktm/Desktop/빅데이터/comment.txt")
print(dat)
tSource <- VectorSource(dat)
tSource
myCor <- Corpus(tSource)
myCor
# 05. 전처리가 후, 문서 내용 확인
# tm_map( tolower )
myCorpus <- tm_map(myCorpus, removeNumbers)                     # 숫자 제거
myCorpus <- tm_map(myCorpus, removePunctuation)                 # 특수 문자 제거
myCorpus <- tm_map(myCorpus, stripWhitespace)                   # 공백 제거
inspect(myCor)
myCor[[3]]$content
# 06. 불용어 처리 수행
## stopwords : C:/Users/ktm/Documents/R/win-library/3.5/tm/stopwords
## 나의 라이브러리 위치
.libPaths()
myCorpus <- tm_map(myCorpus, removeWords, stopwords(""))
# 07. 단어 문서 행렬(Term Document Matrix, TDM)
# TF(용어의 빈도수)
# TFIDF
tdm <- TermDocumentMatrix(myCor, control=list(tokenize="scan", wordLengths=c(2,7)))
inspect(tdm)
tdm_M <- as.matrix(tdm)
tdm_M
## 단어 빈도수 구하기
frequency <- rowSums(tdm_M)
frequency
## 정렬
frequency <- sort(frequency, decreasing = T)
frequency    # 단어별 빈도
barplot(frequency[1:20], las=2)   # 막대그래프 그리고
colors()
RColorBrewer::brewer.pal.info
## 워드 클라우드
library(wordcloud)
w1 <- names(frequency)   # 단어별- 이름
pal <- brewer.pal(8, "Dark2") # 색 지정
wordcloud(words=w1,       # 이름
freq=frequency, # 빈도수
min.freq=1,     # 표시할 최소 횟수
random.order=F, # 가장 횟수가 많은 단어를 중심으로
random.color=T, # 여러가지 색을 랜덤하게 지정한다.
colors=pal)     # 색지정
library(KoNLP)
library(NIADic)
useNIADic()
removeWord = c("본","ㅡ", "에", "아 ", "왜나와", " 와", "이", "이어", "  다", "거", "ㅎㅎㅎ ", "부른", "넘들 ", " 못피함", "쓰기가")
for (i in seq_along(myCorpus)) {
myCorpus[[i]]$content <- gsub("본", " ", myCorpus[[i]]$content)
myCorpus[[i]]$content <- gsub("ㅡ", " ", myCorpus[[i]]$content)
myCorpus[[i]]$content <- gsub("에", " ", myCorpus[[i]]$content)
myCorpus[[i]]$content <- gsub("아", " ", myCorpus[[i]]$content)
myCorpus[[i]]$content <- gsub("왜나와", " ", myCorpus[[i]]$content)
myCorpus[[i]]$content <- gsub("와", " ", myCorpus[[i]]$content)
myCorpus[[i]]$content <- gsub("이", " ", myCorpus[[i]]$content)
myCorpus[[i]]$content <- gsub("이어", " ", myCorpus[[i]]$content)
myCorpus[[i]]$content <- gsub("다", " ", myCorpus[[i]]$content)
myCorpus[[i]]$content <- gsub("거", " ", myCorpus[[i]]$content)
myCorpus[[i]]$content <- gsub("ㅎㅎㅎ", " ", myCorpus[[i]]$content)
myCorpus[[i]]$content <- gsub("부른", " ", myCorpus[[i]]$content)
myCorpus[[i]]$content <- gsub("넘들", " ", myCorpus[[i]]$content)
myCorpus[[i]]$content <- gsub("못피함", " ", myCorpus[[i]]$content)
myCorpus[[i]]$content <- gsub("쓰기가", " ", myCorpus[[i]]$content)
}
myCorpus <- tm_map(myCorpus, removeWords, removeWord)
wordcloud(words=w1,       # 이름
freq=frequency, # 빈도수
min.freq=1,     # 표시할 최소 횟수
random.order=F, # 가장 횟수가 많은 단어를 중심으로
random.color=T, # 여러가지 색을 랜덤하게 지정한다.
colors=pal)     # 색지정
removeWord = c("본","ㅡ", "에", "아 ", "왜나와", " 와", "이", "이어", "  다", "거", "ㅎㅎㅎ ", "부른", "넘들 ", " 못피함", "쓰기가")
myCorpus <- tm_map(myCorpus, removeWords, removeWord)
wordcloud(words=w1,       # 이름
freq=frequency, # 빈도수
min.freq=1,     # 표시할 최소 횟수
random.order=F, # 가장 횟수가 많은 단어를 중심으로
random.color=T, # 여러가지 색을 랜덤하게 지정한다.
colors=pal)     # 색지정
for (i in seq_along(myCorpus)) {
myCorpus[[i]]$content <- gsub("본", " ", myCorpus[[i]]$content)
myCorpus[[i]]$content <- gsub("ㅡ", " ", myCorpus[[i]]$content)
myCorpus[[i]]$content <- gsub("에", " ", myCorpus[[i]]$content)
myCorpus[[i]]$content <- gsub("아", " ", myCorpus[[i]]$content)
myCorpus[[i]]$content <- gsub("왜나와", " ", myCorpus[[i]]$content)
myCorpus[[i]]$content <- gsub("와", " ", myCorpus[[i]]$content)
myCorpus[[i]]$content <- gsub("이", " ", myCorpus[[i]]$content)
myCorpus[[i]]$content <- gsub("이어", " ", myCorpus[[i]]$content)
myCorpus[[i]]$content <- gsub("다", " ", myCorpus[[i]]$content)
myCorpus[[i]]$content <- gsub("거", " ", myCorpus[[i]]$content)
myCorpus[[i]]$content <- gsub("ㅎㅎㅎ", " ", myCorpus[[i]]$content)
myCorpus[[i]]$content <- gsub("부른", " ", myCorpus[[i]]$content)
myCorpus[[i]]$content <- gsub("넘들", " ", myCorpus[[i]]$content)
myCorpus[[i]]$content <- gsub("못피함", " ", myCorpus[[i]]$content)
myCorpus[[i]]$content <- gsub("쓰기가", " ", myCorpus[[i]]$content)
}
wordcloud(words=w1,       # 이름
freq=frequency, # 빈도수
min.freq=1,     # 표시할 최소 횟수
random.order=F, # 가장 횟수가 많은 단어를 중심으로
random.color=T, # 여러가지 색을 랜덤하게 지정한다.
colors=pal)     # 색지정
myCorpus
myCorpus <- tm_map(myCorpus, removeWords, removeWord)
# 05. 전처리가 후, 문서 내용 확인
# tm_map( tolower )
myCorpus <- tm_map(myCorpus, removeNumbers)                     # 숫자 제거
myCorpus <- tm_map(myCorpus, removePunctuation)                 # 특수 문자 제거
myCorpus <- tm_map(myCorpus, stripWhitespace)                   # 공백 제거
inspect(myCor)
myCor[[3]]$content
# 06. 불용어 처리 수행
## stopwords : C:/Users/ktm/Documents/R/win-library/3.5/tm/stopwords
## 나의 라이브러리 위치
.libPaths()
myCorpus <- tm_map(myCorpus, removeWords, stopwords(""))
# 07. 단어 문서 행렬(Term Document Matrix, TDM)
# TF(용어의 빈도수)
# TFIDF
tdm <- TermDocumentMatrix(myCor, control=list(tokenize="scan", wordLengths=c(2,7)))
inspect(tdm)
tdm_M <- as.matrix(tdm)
tdm_M
## 단어 빈도수 구하기
frequency <- rowSums(tdm_M)
frequency
## 정렬
frequency <- sort(frequency, decreasing = T)
frequency    # 단어별 빈도
barplot(frequency[1:20], las=2)   # 막대그래프 그리고
colors()
RColorBrewer::brewer.pal.info
## 워드 클라우드
library(wordcloud)
w1 <- names(frequency)   # 단어별- 이름
pal <- brewer.pal(8, "Dark2") # 색 지정
wordcloud(words=w1,       # 이름
freq=frequency, # 빈도수
min.freq=1,     # 표시할 최소 횟수
random.order=F, # 가장 횟수가 많은 단어를 중심으로
random.color=T, # 여러가지 색을 랜덤하게 지정한다.
colors=pal)     # 색지정
library(KoNLP)
library(NIADic)
useNIADic()
removeWord = c("본","ㅡ", "에", "아 ", "왜나와", " 와", "이", "이어", "  다", "거", "ㅎㅎㅎ ", "부른", "넘들 ", " 못피함", "쓰기가")
for (i in seq_along(myCorpus)) {
myCorpus[[i]]$content <- gsub("본", " ", myCorpus[[i]]$content)
myCorpus[[i]]$content <- gsub("ㅡ", " ", myCorpus[[i]]$content)
myCorpus[[i]]$content <- gsub("에", " ", myCorpus[[i]]$content)
myCorpus[[i]]$content <- gsub("아", " ", myCorpus[[i]]$content)
myCorpus[[i]]$content <- gsub("왜나와", " ", myCorpus[[i]]$content)
myCorpus[[i]]$content <- gsub("와", " ", myCorpus[[i]]$content)
myCorpus[[i]]$content <- gsub("이", " ", myCorpus[[i]]$content)
myCorpus[[i]]$content <- gsub("이어", " ", myCorpus[[i]]$content)
myCorpus[[i]]$content <- gsub("다", " ", myCorpus[[i]]$content)
myCorpus[[i]]$content <- gsub("거", " ", myCorpus[[i]]$content)
myCorpus[[i]]$content <- gsub("ㅎㅎㅎ", " ", myCorpus[[i]]$content)
myCorpus[[i]]$content <- gsub("부른", " ", myCorpus[[i]]$content)
myCorpus[[i]]$content <- gsub("넘들", " ", myCorpus[[i]]$content)
myCorpus[[i]]$content <- gsub("못피함", " ", myCorpus[[i]]$content)
myCorpus[[i]]$content <- gsub("쓰기가", " ", myCorpus[[i]]$content)
}
myCorpus <- tm_map(myCorpus, removeWords, removeWord)
wordcloud
wordcloud(words=w1,       # 이름
freq=frequency, # 빈도수
min.freq=1,     # 표시할 최소 횟수
random.order=F, # 가장 횟수가 많은 단어를 중심으로
random.color=T, # 여러가지 색을 랜덤하게 지정한다.
colors=pal)     # 색지정
myCorpus <- tm_map(myCorpus, removeWords, removeWord)
wordcloud(words=w1,       # 이름
freq=frequency, # 빈도수
min.freq=1,     # 표시할 최소 횟수
random.order=F, # 가장 횟수가 많은 단어를 중심으로
random.color=T, # 여러가지 색을 랜덤하게 지정한다.
colors=pal)     # 색지정
library(tm)
dat <- readLines("C:/Users/ktm/Desktop/빅데이터/comment.txt")
print(dat)
tSource <- VectorSource(dat)
tSource
myCor <- Corpus(tSource)
myCor
# 05. 전처리가 후, 문서 내용 확인
# tm_map( tolower )
myCorpus <- tm_map(myCorpus, removeNumbers)                     # 숫자 제거
myCorpus <- tm_map(myCorpus, removePunctuation)                 # 특수 문자 제거
myCorpus <- tm_map(myCorpus, stripWhitespace)                   # 공백 제거
removeWord = c("본","ㅡ", "에", "아 ", "왜나와", " 와", "이", "이어", "  다", "거", "ㅎㅎㅎ ", "부른", "넘들 ", " 못피함", "쓰기가")
myCorpus <- tm_map(myCorpus, removeWords, removeWord)
inspect(myCor)
myCor[[3]]$content
# 06. 불용어 처리 수행
## stopwords : C:/Users/ktm/Documents/R/win-library/3.5/tm/stopwords
## 나의 라이브러리 위치
.libPaths()
myCorpus <- tm_map(myCorpus, removeWords, stopwords(""))
# 07. 단어 문서 행렬(Term Document Matrix, TDM)
# TF(용어의 빈도수)
# TFIDF
tdm <- TermDocumentMatrix(myCor, control=list(tokenize="scan", wordLengths=c(2,7)))
inspect(tdm)
tdm_M <- as.matrix(tdm)
tdm_M
## 단어 빈도수 구하기
frequency <- rowSums(tdm_M)
frequency
## 정렬
frequency <- sort(frequency, decreasing = T)
frequency    # 단어별 빈도
barplot(frequency[1:20], las=2)   # 막대그래프 그리고
colors()
RColorBrewer::brewer.pal.info
## 워드 클라우드
library(wordcloud)
w1 <- names(frequency)   # 단어별- 이름
pal <- brewer.pal(8, "Dark2") # 색 지정
wordcloud(words=w1,       # 이름
freq=frequency, # 빈도수
min.freq=1,     # 표시할 최소 횟수
random.order=F, # 가장 횟수가 많은 단어를 중심으로
random.color=T, # 여러가지 색을 랜덤하게 지정한다.
colors=pal)     # 색지정
library(caret)
set.seed(1712)
inTrain <- createDataPartition(y = iris$Species, p = 0.7, list = F)
training <- iris[inTrain,]
testing <- iris[-inTrain,]
training
testing
# 3.2 표준화
# K-means 군집 분석은 관측치 간의 거리를 이용하기 때문에
# 변수의 단위가 결과에 큰 영향을 미칩니다.
# 그래서 변수를 표준화 하는 작업이 필요한데요,
# scale 함수를 사용해서 표준화
training.data <- scale(training[-5])
summary(training.data)
# 3.3
# 모델 작성 - 3개의 군집으로 나누기
iris.kmeans <- kmeans(training.data[,-5], centers = 3, iter.max = 10000)
iris.kmeans$centers
# 3.4
# 군집 분석 결과를 training 데이터셋에 할당하고, 결과를 확인
# 결과를 보니 setosa는 잘 분류해 내었지만 versicolor나 virginica 종은 잘 구분해 내지 못함.
training$cluster <- as.factor(iris.kmeans$cluster)
qplot(Petal.Width, Petal.Length, colour = cluster, data = training)
table(training$Species, training$cluster)
# 3.5
# K-means 군집분석에서 군집 중심의 갯수를 결정하는 방법
# 몇개의 군집 중심이 적당한지 결정하는 방법에는 여러가지가 있습니다.
# 그중 자주 사용하는 NbClust 패키지를 사용하는 방법과
# 군집 내  sum of squares를 사용하는 방법
install.packages("NbClust")
library(NbClust)
nc <- NbClust(training.data, min.nc = 2, max.nc = 15, method = "kmeans")
setwd("C:/Users/ktm/Desktop/빅데이터/Auction_master_kr (수정)")
df <- read.csv("Auction_master_test2.csv")
str(df)
dim(df)
head(df)
names(df)
sel = c("Claim_price", "Appraisal_date", "Total_land_gross_area", "Total_land_real_area", "Total_building_area", "Total_appraisal_price","Minimum_sales_price", "First_auction_date","Final_auction_date", "Preserve_regist_date", "Close_date")
trA_subset <- df[ , sel]
setwd("C:/Users/ktm/Desktop/빅데이터/Auction_master_kr (수정)")
df <- read.csv("Auction_master_test2.csv")
str(df)
dim(df)
head(df)
names(df)
sel = c("Claim_price", "Appraisal_date", "Total_land_gross_area", "Total_land_real_area", "Total_building_area", "Total_appraisal_price","Minimum_sales_price", "First_auction_date","Final_auction_date", "Preserve_regist_date", "Close_date")
trA_subset <- df[ , sel]
plot(trA_subset)
tr_cor = cor(trA_subset)
install.packages("corrplot")
library(corrplot)
tr_cor = cor(trA_subset)
trA_subset
DF <- data.frame(temp=1:7, count=seq(1,7,by=1))
DF <- data.frame(temp=1:7, count=seq(1,7,by=1))
plot(Claim_price ~ Close_date, data=DF, pch=20, col="red" )
plot(Claim_price ~ Close_date, data=df, pch=20, col="red" )
dat <- readLines("data/comment.txt")
dat <- readLines("./data/comment.txt")
setwd("E:/bigdata/Github/todaylearn/data")
dat <- readLines("./data/comment.txt")
setwd("E:/bigdata/Github/todaylearn/data")
dat <- readLines("comment.txt")
print(dat)
# 05. 전처리가 후, 문서 내용 확인
# tm_map( tolower )
myCorpus <- tm_map(myCorpus, removeNumbers)                     # 숫자 제거
myCorpus <- tm_map(myCorpus, removePunctuation)                 # 특수 문자 제거
inspect(myCor)
myCor[[3]]$content
# 06. 단어 문서 행렬(Term Document Matrix, TDM)
# 단어의 빈도를 확인하기 위해서
# 단어와 단어간의 연관성을 확인하기 위해서
# TF(용어의 빈도수)
# TFIDF
tdm <- TermDocumentMatrix(myCor, control=list(tokenize="scan", wordLengths=c(2,7)))
inspect(tdm)
# 06. 단어 문서 행렬(Term Document Matrix, TDM)
# 단어의 빈도를 확인하기 위해서
# 단어와 단어간의 연관성을 확인하기 위해서
# TF(용어의 빈도수)
# TFIDF
tdm <- TermDocumentMatrix(myCor, control=list(tokenize="scan", wordLengths=c(2,2)))
inspect(tdm)
# 06. 단어 문서 행렬(Term Document Matrix, TDM)
# 단어의 빈도를 확인하기 위해서
# 단어와 단어간의 연관성을 확인하기 위해서
# TF(용어의 빈도수)
# TFIDF
# 한글은 한글자를 두글자로 봄.
tdm <- TermDocumentMatrix(myCor, control=list(tokenize="scan", wordLengths=c(2,8)))
inspect(tdm)
tdm_M <- as.matrix(tdm)
tdm_M
## 단어 빈도수 구하기
frequency <- rowSums(tdm_M)
frequency
## 07. 정렬
frequency <- sort(frequency, decreasing = T)
frequency    # 단어별 빈도
barplot(frequency[1:20], las=2)   # 막대그래프 그리고
## 워드 클라우드
library(wordcloud)
w1 <- names(frequency)   # 단어별- 이름
pal <- brewer.pal(8, "Dark2") # 색 지정
wordcloud(words=w1,       # 이름
freq=frequency, # 빈도수
min.freq=1,     # 표시할 최소 횟수
random.order=F, # 가장 횟수가 많은 단어를 중심으로
random.color=T, # 여러가지 색을 랜덤하게 지정한다.
colors=pal)     # 색지정
wordcloud(words=w1,       # 이름
freq=frequency, # 빈도수
min.freq=1,     # 표시할 최소 횟수
random.order=F, # 가장 횟수가 많은 단어를 중심으로
random.color=T, # 여러가지 색을 랜덤하게 지정한다.
colors=pal)     # 색지정
names(frequency)
wordName <- names(frequency)   # 단어별- 이름
pal <- brewer.pal(8, "Dark2") # 색 지정
wordcloud(words=wordName,       # 이름
freq=frequency, # 빈도수
min.freq=1,     # 표시할 최소 횟수
random.order=F, # 가장 횟수가 많은 단어를 중심으로
random.color=T, # 여러가지 색을 랜덤하게 지정한다.
colors=pal)     # 색지정
wordcloud(words=wordName,       # 이름
freq=frequency, # 빈도수
min.freq=1,     # 표시할 최소 횟수
random.order=F, # 가장 횟수가 많은 단어를 중심으로
random.color=T, # 여러가지 색을 랜덤하게 지정한다.
colors=pal)     # 색지정
